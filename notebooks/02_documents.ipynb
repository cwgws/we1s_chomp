{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # STEP 2: SCRAPE DOCUMENTS\n",
    "\n",
    " <h4>This notebook will collect document metadata from scraped responses.</h4>\n",
    "\n",
    "It supports the Wordpress API (reference here: https://developer.wordpress.org/rest-api/) and the Google Custom Search Engine API (reference here: https://developers.google.com/custom-search/v1/) out of the box, though other APIs can be added. For more information and further instructions, consult the Chomp documentation at https://github.com/seangilleran/we1s_chomp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## INFO\n",
    " \n",
    "__authors__    = 'Sean Gilleran'  \n",
    "__copyright__  = 'copyright 2019, The WE1S Project'  \n",
    "__license__    = 'MIT'  \n",
    "__version__    = '0.1.0'  \n",
    "__email__      = 'sgilleran@ucsb.edu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added URL stopword: \".pdf\".\n",
      "Added URL stopword: \".doc\".\n",
      "Added URL stopword: \".docx\".\n",
      "Added URL stopword: \".odf\".\n",
      "Added URL stopword: \"/author/\".\n",
      "Added URL stopword: \"/contributor/\".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import getenv\n",
    "from pathlib import Path\n",
    "\n",
    "from we1s_chomp import google, wordpress\n",
    "from we1s_chomp.model import Document\n",
    "from we1s_chomp.web import Browser\n",
    "\n",
    "project_dir = Path.home() / \"write\" / \"dev\" / \"we1s_chomp\"\n",
    "url_stopwords_file = project_dir / \"notebooks\" / \"url_stopwords.txt\"\n",
    "\n",
    "#############################################################################\n",
    "# NOTE 7/31 -- TEMPORARY PROBLEM W/ GETTING AN ENVIRONMENT VARIABLE --\n",
    "# DO NOT STORE URLS OR API KEYS IN GIT REPO!!!!\n",
    "#grid_url = getenv(\"CHOMP_SELENIUM_GRID_URL\")\n",
    "grid_url = \"http://harbor.english.ucsb.edu:4444\"\n",
    "#############################################################################\n",
    "\n",
    "url_stops = set()\n",
    "\n",
    "# Get stopwords.\n",
    "url_stopwords = set()\n",
    "with open(url_stopwords_file, encoding=\"utf-8\") as txtfile:\n",
    "    for line in txtfile.readlines():\n",
    "        stopword = line.strip()\n",
    "        if stopword != \"\":\n",
    "            url_stopwords.add(stopword)\n",
    "            print(f'Added URL stopword: \"{stopword}\".')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## DATA DIRECTORIES\n",
    "\n",
    " Chomp will import and export JSON files to these directories, using them not\n",
    " only to set up and store the results of a collection run and but also to keep\n",
    " track of metadata, duplicate URLs, page numbers, etc. as it goes.\n",
    "\n",
    " <p style=\"color:red;\">Because collection is a time- and resource-intensive\n",
    " task, it is preferable to keep these metadata files in a single location and\n",
    " allow Chomp to manage them internally rather than modifying or deleting them\n",
    " between each job.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sources from /home/jovyan/write/dev/we1s_chomp/data/json/sources.\n",
      "Loading queries from /home/jovyan/write/dev/we1s_chomp/data/json/queries.\n",
      "Loading responses from /home/jovyan/write/dev/we1s_chomp/data/json/responses.\n",
      "Saving documents to /home/jovyan/write/dev/we1s_chomp/data/json/documents.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_dir = project_dir / \"data\" / \"json\" / \"queries\"\n",
    "source_dir = project_dir / \"data\" / \"json\" / \"sources\"\n",
    "response_dir = project_dir / \"data\" / \"json\" / \"responses\"\n",
    "document_dir = project_dir / \"data\" / \"json\" / \"documents\"\n",
    "\n",
    "# Make document directory if it does not already exist.\n",
    "if not document_dir.exists():\n",
    "    document_dir.mkdir(parents=True)\n",
    "\n",
    "print(f\"Loading sources from {source_dir}.\")\n",
    "print(f\"Loading queries from {query_dir}.\")\n",
    "print(f\"Loading responses from {response_dir}.\")\n",
    "print(f\"Saving documents to {document_dir}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## BROWSE: Search responses for keywords\n",
    "\n",
    "Choose `search_text` to filter available response files. If you are searching for a specific word or phrase, enter it WITHIN the single quotes below. Note that you will be searching the filenames of the JSON files stored on in the `response_dir` path (usually `data/json/responses`). If you want to simply list all of the available responses, change the value of the `search_text` variable below to `None` WITHOUT single quotes (so the line should read `search_text = None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_text = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell and review the results. The default is to search through the `data/json/responses` directory. If your data is in a different location on harbor, change the `response_dir` variable above to the directory you want to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_list = [\n",
      "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_0\",\n",
      "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_1\",\n",
      "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_2\",\n",
      "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_3\",\n",
      "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_4\",\n",
      "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_5\",\n",
      "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_6\",\n",
      "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_7\",\n",
      "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_0\",\n",
      "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_1\",\n",
      "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_2\",\n",
      "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_3\",\n",
      "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_4\",\n",
      "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_5\",\n",
      "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_6\",\n",
      "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_7\",\n",
      "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_8\",\n",
      "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_9\",\n",
      "]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"response_list = [\")\n",
    "for filename in response_dir.glob(\"*.json\"):\n",
    "    with open(filename, encoding=\"utf-8\") as jsonfile:\n",
    "        name = json.load(jsonfile).get(\"name\", \"\")\n",
    "    if not name or \"chomp-response\" not in name:\n",
    "        continue\n",
    "    print('    \"' + name + '\",')\n",
    "print(\"]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIST: Define which queries will be scraped\n",
    "\n",
    "Copy the entire cell output above and replace the `response_list` array in the following cell. Each response name should be surrounded by quotes, and after each name there should be a comma (for the last filename in the list it doesn't matter if you include the comma or not).\n",
    "\n",
    "Don't forget to run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_list = [\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_0\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_1\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_2\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_3\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_4\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_5\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_6\",\n",
    "    \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_7\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_0\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_1\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_2\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_3\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_4\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_5\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_6\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_7\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_8\",\n",
    "    \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_9\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## IMPORT & TEST RESPONSES\n",
    "\n",
    " Let's take a second here to make sure our responses are all in good shape--\n",
    " that the dates are all correct and that they all connect to a proper source.\n",
    " That way there won't be any surprises later.\n",
    "\n",
    " Run this cell and check for any errors in the output. Make sure that the\n",
    " number of responses imported is equal to the number of responses you intended\n",
    " to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_0\".\n",
      "Imported \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_1\".\n",
      "Imported \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_2\".\n",
      "Imported \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_3\".\n",
      "Imported \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_4\".\n",
      "Imported \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_5\".\n",
      "Imported \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_6\".\n",
      "Imported \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_7\".\n",
      "Imported \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_0\".\n",
      "Imported \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_1\".\n",
      "Imported \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_2\".\n",
      "Imported \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_3\".\n",
      "Imported \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_4\".\n",
      "Imported \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_5\".\n",
      "Imported \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_6\".\n",
      "Imported \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_7\".\n",
      "Imported \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_8\".\n",
      "Imported \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_9\".\n",
      "18 responses imported out of 18 total.\n",
      "Everything looks good so far!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from we1s_chomp import clean, db\n",
    "\n",
    "responses = []\n",
    "for response_name in response_list:\n",
    "    response = db.load_response(response_name, response_dir)\n",
    "    if not db.load_source(response.source, source_dir):\n",
    "        print(f'WRN: \"{response.source}\" not found, skipping \"{response.name}\".')\n",
    "        continue\n",
    "    if not db.load_query(response.query, query_dir):\n",
    "        print(f'WRN: \"{response.query}\" not found, skipping \"{response.name}\".')\n",
    "        continue\n",
    "    responses.append(response)\n",
    "    print(f'Imported \"{response.name}\".')\n",
    "print(f\"{len(responses)} responses imported out of {len(response_list)} total.\")\n",
    "if len(responses) == len(response_list):\n",
    "    print(\"Everything looks good so far!\\n\\n\")\n",
    "else:\n",
    "    print(\"Hmm, does that seem right to you? Double-check!\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## LOAD URL STOPS (Optional)\n",
    "\n",
    " Load previously collected URLs. Skipping this step will force all responses to be re-collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in responses:\n",
    "    for document_name in response.documents:\n",
    "        document = db.load_document(document_name, document_dir)\n",
    "        url_stops.add(document.url)\n",
    "print(f\"Added {len(url_stops)} URLs to URL stop list.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # GET DOCUMENTS\n",
    "\n",
    " Use the queries to start scraping responses.\n",
    " \n",
    " <h3 style=\"color:red;font-weight:bold\">Pay close attention to errors here--many things can go wrong when dealing with the web!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%%time\n",
    "total = 0\n",
    "\n",
    "# Start browser connection. We need this for Google documents.\n",
    "browser = Browser(grid_url)\n",
    "\n",
    "# We need to restart document count whenever we get a new query; that means\n",
    "# keeping track of the old query.\n",
    "last_query = db.load_query(responses[0].query, query_dir)\n",
    "count = no_exact_match_count = 0\n",
    "\n",
    "for response in responses:\n",
    "\n",
    "    # Load associated source & query.\n",
    "    source = db.load_source(response.source, source_dir)\n",
    "    query = db.load_query(response.query, query_dir)\n",
    "    \n",
    "    # Reset count on new query.\n",
    "    if last_query.name != query.name:\n",
    "        count = no_exact_match_count = 0\n",
    "\n",
    "    # Select collection API.\n",
    "    api = response.chompApi\n",
    "\n",
    "    # Wordpress API ##########################################################\n",
    "    if api == \"wordpress\":\n",
    "        print(f'\\nCollecting \"{response.name}\" via Wordpress API...')\n",
    "        documents_raw = wordpress.get_metadata(\n",
    "            response=response.content,\n",
    "            query_str=query.query_str,\n",
    "            start_date=query.start_date,\n",
    "            end_date=query.end_date,\n",
    "            url_stops=url_stops,\n",
    "            url_stopwords=url_stopwords\n",
    "        )\n",
    "\n",
    "    # Google API #############################################################\n",
    "    else:\n",
    "        print(f'\\nCollecting \"{response.name}\" via Google API...')\n",
    "        documents_raw = google.get_metadata(\n",
    "            response=response.content,\n",
    "            query_str=query.query_str,\n",
    "            start_date=query.start_date,\n",
    "            end_date=query.end_date,\n",
    "            url_stops=url_stops,\n",
    "            url_stopwords=url_stopwords,\n",
    "            browser=browser,\n",
    "        )\n",
    "\n",
    "    if not documents_raw:\n",
    "        print(\"ERR: No results or connection error!\")\n",
    "        continue\n",
    "\n",
    "    # Loop over each document we get back and save it.\n",
    "    for doc in documents_raw:\n",
    "        if not doc:\n",
    "            print(\"WRN: No document found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Parse result.\n",
    "        name = \"_\".join(\n",
    "            [\n",
    "                \"chomp\",\n",
    "                query.name,\n",
    "                str(count if not doc[\"no_exact_match\"] else no_exact_match_count)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Documents that do not explicitly contain the search term may require\n",
    "        # special handling--set them aside.\n",
    "        if doc[\"no_exact_match\"]:\n",
    "            name += \"(no-exact-match)\"\n",
    "\n",
    "        document = Document(\n",
    "            name=name,\n",
    "            title=doc[\"title\"],\n",
    "            shortTitle=name,\n",
    "            chompApi=api,\n",
    "            url=doc[\"url\"],\n",
    "            pub_date=doc[\"pub_date\"],\n",
    "            content_html=doc[\"content_html\"],\n",
    "            content=doc[\"content\"],\n",
    "            pub=source.title,\n",
    "            copyright=source.copyright,\n",
    "            source=source.name,\n",
    "            query=query.name,\n",
    "            response=response.name,\n",
    "        )\n",
    "\n",
    "        if not doc[\"no_exact_match\"]:\n",
    "            count += 1\n",
    "        else:\n",
    "            no_exact_match_count += 1\n",
    "        total += 1\n",
    "        db.save_document(document, document_dir)\n",
    "\n",
    "        # Update response.\n",
    "        response.documents.add(document.name)\n",
    "        db.save_response(response, response_dir)\n",
    "\n",
    "        # Update query.\n",
    "        query.documents.add(document.name)\n",
    "        db.save_query(query, query_dir)\n",
    "\n",
    "        # Update source.\n",
    "        source.documents.add(document.name)\n",
    "        db.save_source(source, source_dir)\n",
    "        \n",
    "        # Save previous query.\n",
    "        last_query = query\n",
    "\n",
    "        print(f\"- {document.url}\")\n",
    "    print(f\"Done! Got {count + no_exact_match_count} documents from this response.\\n\\n\")\n",
    "print(f\"\\nAll responses complete! Got a total of {total} documents.\\n\\n\")\n",
    "print(\"\\n\\n----------Time----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_0\" via Wordpress API...\n",
      "- https://we1s.ucsb.edu/bibliography-topic-modeling-in-digital-humanities/\n",
      "- https://we1s.ucsb.edu/research-resources/we1s-bibliography/bibliography-digital-humanities/\n",
      "- https://we1s.ucsb.edu/tools/topic-model-interpretation-protocol/\n",
      "- https://we1s.ucsb.edu/events/summer-research-camps/summer-research-camp-2019/\n",
      "- https://we1s.ucsb.edu/events/directions-and-maps/directions-to-the-dahc-digital-arts-humanities-commons/\n",
      "- https://we1s.ucsb.edu/events/summer-research-camps/summer-research-camp-2019/2019-csun-summer-research-camp-application/\n",
      "- https://we1s.ucsb.edu/about/team-2/\n",
      "- https://we1s.ucsb.edu/research-resources/we1s-bibliography/bibliography-history-of-humanities/\n",
      "- https://we1s.ucsb.edu/research-resources/we1s-bibliography/bibliography-global-humanities/\n",
      "- https://we1s.ucsb.edu/research-resources/we1s-bibliography/bibliography-humanities/\n",
      "Done! Got 10 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_1\" via Wordpress API...\n",
      "- https://we1s.ucsb.edu/developer-resources/orientation-materials-for-we1s-project-participants/\n",
      "- https://we1s.ucsb.edu/events/directions-and-maps/directions-to-dahc/\n",
      "- https://we1s.ucsb.edu/developer-resources/we1s-social-media-guide/\n",
      "- https://we1s.ucsb.edu/team-bios/giorgina-paiella/\n",
      "- https://we1s.ucsb.edu/research-outputs/sample-topic-models-from-summer-research-camp-2018/\n",
      "- https://we1s.ucsb.edu/team-bios/raymond-steding/\n",
      "- https://we1s.ucsb.edu/team-bios/rebecca-baker/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/united-kingdom-news-sources/\n",
      "- https://we1s.ucsb.edu/team-bios/katie-wolf/\n",
      "- https://we1s.ucsb.edu/team-bios/maureen-nyhan/\n",
      "Done! Got 20 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_2\" via Wordpress API...\n",
      "- https://we1s.ucsb.edu/team-bios/joyce-mcgee-brummet/\n",
      "- https://we1s.ucsb.edu/team-bios/samantha-wallace/\n",
      "- https://we1s.ucsb.edu/team-bios/mauro-carassai/\n",
      "- https://we1s.ucsb.edu/team-bios/nazanin-keynejad/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/central-and-south-america-area-of-focus-report/\n",
      "- https://we1s.ucsb.edu/team-bios/abigail-droge/\n",
      "- https://we1s.ucsb.edu/team-bios/dan-costa-baciu/\n",
      "- https://we1s.ucsb.edu/events/advisory-board-meeting-aug-3-4-2018/\n",
      "- https://we1s.ucsb.edu/curriculum-lab/\n",
      "- https://we1s.ucsb.edu/events/summer-research-camps/summer-research-camp-2018/general-info/\n",
      "Done! Got 30 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_3\" via Wordpress API...\n",
      "- https://we1s.ucsb.edu/developer-resources/area-of-focus-report-template/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/hispanophone-caribbean-news-sources/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/political-orientation-news-sources/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/u-s-spanish-language-news-sources/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/u-s-online-news-sources/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/middle-eastern-sources/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/diverse-populations/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/european-news-sources/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/canadian-news-sources/\n",
      "- https://we1s.ucsb.edu/research-outputs/scoping-research-reports/newspaper-corpus-design-and-representativeness-report/\n",
      "Done! Got 40 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_4\" via Wordpress API...\n",
      "- https://we1s.ucsb.edu/research-outputs/scoping-research-reports/the-edition/\n",
      "- https://we1s.ucsb.edu/research-outputs/scoping-research-reports/newspaper-studies/\n",
      "- https://we1s.ucsb.edu/research-outputs/scoping-research-reports/corpus-linguistics/\n",
      "- https://we1s.ucsb.edu/research-outputs/scoping-research-reports/media-impact/\n",
      "- https://we1s.ucsb.edu/research-outputs/scoping-research-reports/the-canon/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/african-news-sources/\n",
      "- https://we1s.ucsb.edu/corpus-sources/area-of-focus-research-reports/\n",
      "- https://we1s.ucsb.edu/events/summer-research-camps/summer-research-camp-2018/\n",
      "- https://we1s.ucsb.edu/events/summer-research-camps/\n",
      "- https://we1s.ucsb.edu/research-resources/we1s-bibliography/\n",
      "Done! Got 50 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_5\" via Wordpress API...\n",
      "- https://we1s.ucsb.edu/postdoctoral-scholars/postdoctoral-scholar-for-the-whatevery1says-project-job-call/\n",
      "- https://we1s.ucsb.edu/about/advisory-board/\n",
      "- https://we1s.ucsb.edu/about/we1s-prospectus/\n",
      "- https://we1s.ucsb.edu/research-outputs/\n",
      "- https://we1s.ucsb.edu/about/\n",
      "- https://we1s.ucsb.edu/\n",
      "Done! Got 56 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_6\" via Wordpress API...\n",
      "- https://we1s.ucsb.edu/we1s-summer-camp-retrospective/\n",
      "- https://we1s.ucsb.edu/broadcast-internet-media-team-starter-kit/\n",
      "- https://we1s.ucsb.edu/united-kingdom-team-starter-kit/\n",
      "- https://we1s.ucsb.edu/u-s-and-canada-team-starter-kit/\n",
      "- https://we1s.ucsb.edu/we1s-kicks-off-summer-2018-research-camp/\n",
      "- https://we1s.ucsb.edu/postdoctoral-scholars-dan-costa-baciu-and-abigail-droge-to-join-we1s-project/\n",
      "- https://we1s.ucsb.edu/recording-of-we1s-markdown-and-github-workshop/\n",
      "- https://we1s.ucsb.edu/we1s-opens-search-for-two-postdoctoral-scholars/\n",
      "- https://we1s.ucsb.edu/we1s-announces-its-advisory-board/\n",
      "- https://we1s.ucsb.edu/csun-interviews-scott-kleinman-in-announcement-of-we1s-grant/\n",
      "Done! Got 66 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_we1s_humanities_2014-01-01_2019-12-31_7\" via Wordpress API...\n",
      "- https://we1s.ucsb.edu/we1s-markdown-and-github-workshop/\n",
      "- https://we1s.ucsb.edu/u-miami-announces-participation-in-the-we1s-project/\n",
      "- https://we1s.ucsb.edu/mellon-foundation-awards-we1s-1-1-million-in-funding/\n",
      "Done! Got 69 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_0\" via Google API...\n",
      "- https://libcom.org/library/world-mathematical-curve-misuse-mathematics-humanities-social-sciences\n",
      "- http://libcom.org/library/against-humanities-self-consciousness-corporate-university\n",
      "- https://libcom.org/library/s01e003-new-college-humanities-future-uk-he\n",
      "- https://libcom.org/library/s01e004-new-movements-new-unions\n",
      "- https://libcom.org/library/s01e002-democratic-paradox-discussing-politics-chantal-mouffe\n",
      "- https://libcom.org/library/issue-2-theory-practice\n",
      "Done! Got 6 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_1\" via Google API...\n",
      "- https://libcom.org/library/no-capital-projects-end-capital\n",
      "- https://libcom.org/news/interview-edinburgh-university-students-under-occupation-25112010\n",
      "- https://libcom.org/library/fight-university-cuts-bust-myths\n",
      "- http://libcom.org/library/state-against-state\n",
      "- https://libcom.org/blog/student-protests-escalating-amsterdam-25022015\n",
      "- https://libcom.org/blog/education-round-21112-21012012\n",
      "- https://libcom.org/blog/bloated-corpses-institutional-limits-interview-mark-paschal-13082012\n",
      "Done! Got 13 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_2\" via Google API...\n",
      "- https://libcom.org/tags/mark-paschal\n",
      "- https://libcom.org/library/hoisting-%E2%80%9Cknowledge-bank%E2%80%9D-its-own-petard-world-bank-%E2%80%9Cdouble-crisis%E2%80%9D-african-universities\n",
      "- https://libcom.org/library/when-professors-strip-camera\n",
      "- https://libcom.org/history/nato-bombing-yugoslavia-noam-chomsky\n",
      "- https://libcom.org/library/notes-acknowledgements\n",
      "- https://libcom.org/library/life-struggle-farewell-herman-gorter-anton-pannekoek\n",
      "- https://libcom.org/library/i-wouldnt-want-my-anarchist-friends-be-charge-nuclear-power-station-david-harvey-anarchi\n",
      "Done! Got 20 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_3\" via Google API...\n",
      "- https://libcom.org/news/france-university-students-continue-strikes-12112007\n",
      "- https://libcom.org/library/sexualization-social-relations\n",
      "- https://libcom.org/library/debt-debt-debt\n",
      "- https://libcom.org/library/anarcho-syndicalism-new-world-shell-old\n",
      "- https://libcom.org/library/novara-discussion-danny-hayward-vulgar-keynesianismz\n",
      "- https://libcom.org/blog/right-education-eviction-clashes-torino-08112012\n",
      "- https://libcom.org/library/novara-discussion-mark-fisher-capitalist-realism\n",
      "Done! Got 27 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_4\" via Google API...\n",
      "- https://libcom.org/library/homelessness-age-austerity\n",
      "- https://libcom.org/library/hegels-science-logic-demonstrated-typical-mistakes-bourgeois-science-gegenstandpunkt\n",
      "- https://libcom.org/library/proudhons-principle-federation\n",
      "- https://libcom.org/library/s01e025-uneconomics\n",
      "- https://libcom.org/library/s01e010-english-riots-%E2%80%93-addendum\n",
      "- https://libcom.org/library/s01e006-struggles-space-%E2%80%93-neo-liberal-city\n",
      "- https://libcom.org/library/s01e021-happy-new-year-predictions-2015-%E2%80%93-%E2%80%98austerity-dysphoria%E2%80%99\n",
      "Done! Got 34 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_5\" via Google API...\n",
      "- https://libcom.org/blog/%E2%80%9Cpower-grassroots%E2%80%9D-reflections-step-alternative-education-14042016\n",
      "- https://libcom.org/library/s01e014-discussion-billy-bragg\n",
      "- https://libcom.org/library/s01e023-anarchist-practice-%E2%80%98new-social-movements%E2%80%99-after-1989\n",
      "- https://libcom.org/library/s01e012-discussing-occupy-everything\n",
      "- https://libcom.org/library/s01e007-strikes-direct-action-new-technologies\n",
      "- https://libcom.org/library/bullshit-jobs\n",
      "- https://libcom.org/library/s01e013-what-role-intellectual-within-crisis\n",
      "- https://libcom.org/library/s01e019-pirates-niger-delta\n",
      "- https://libcom.org/library/s01e001-memetics-protest-ryan-gallagher-dsg\n",
      "- https://libcom.org/library/s01e018-conversation-real-social-network\n",
      "Done! Got 44 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_6\" via Google API...\n",
      "- https://libcom.org/news/italy-fall-empire-13112011\n",
      "- https://libcom.org/library/s01e017-student-movements-millbank-1-year-later\n",
      "- https://libcom.org/library/sokal-hoax-physicist-experiments-cultural-studies-alan-sokal\n",
      "- https://libcom.org/library/s01e015-after-future-conversation-franco-%E2%80%98bifo%E2%80%99-berardi\n",
      "- https://libcom.org/library/critique-bourgeois-science-microeconomics-explanation-value-invention-marginal-utility-m\n",
      "- http://libcom.org/blog/artificial-intelligence-tech-workers-universal-income-interview-13102015?page=1\n",
      "- https://libcom.org/library/workerism-autonomia-lessons-italian-left-what-can-2011-learn-italy-1970s\n",
      "Done! Got 51 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_7\" via Google API...\n",
      "- https://libcom.org/blog/idea-communism-17012012\n",
      "- https://libcom.org/news/more-student-protests-sweep-italy-30112010\n",
      "- https://libcom.org/blog/collapse-enrolments-italian-universities-22022013\n",
      "- https://libcom.org/library/sokal-hoax-transgressing-boundaries-towards-transformative-hermeneutics-quantum-gravity-\n",
      "- http://libcom.org/library/harry-cleaver-debates-hillel-ticktin-capitalisms-present-crisis-danger-opportunity\n",
      "- https://libcom.org/library/rough-notes-hegels-science-logic-dunayevskaya\n",
      "- https://libcom.org/library/learning-struggle-my-story-between-workerism-feminism-leopoldina-fortunati\n",
      "Done! Got 58 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_8\" via Google API...\n",
      "- https://libcom.org/news/italy-occupy-everywhere-occupy-everything-november-17-17112011\n",
      "- https://libcom.org/blog/human-nature-communism-16062018\n",
      "- https://libcom.org/library/%E2%80%98-chomsky-puzzle%E2%80%99-includes-interviews-noam-chomsky-chris-knight-tom-wolfe\n",
      "- https://libcom.org/library/wedge-document-intelligent-design-exposed\n",
      "- http://libcom.org/blog/two-weeks-spent-in-rennes/05/27/2006\n",
      "Done! Got 63 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "Collecting \"chomp-response_libcom-org_humanities_2000-01-01_2019-12-31_9\" via Google API...\n",
      "- http://libcom.org/library/irrelevancies-beeching\n",
      "- https://libcom.org/news/interview-with-member-sussex-university-sortusout-campaign-11052006\n",
      "- https://libcom.org/blog/end-repression-iww-union-supports-chile%E2%80%99s-students-fight-education-29082012\n",
      "- https://libcom.org/history/memories-metropolitan-indian\n",
      "- https://libcom.org/library/neighborhood-new-factory\n",
      "Done! Got 68 documents from this response.\n",
      "\n",
      "\n",
      "\n",
      "All responses complete! Got a total of 137 documents.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------Time----------\n",
      "CPU times: user 17.2 s, sys: 2.94 s, total: 20.1 s\n",
      "Wall time: 10min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No content found in HTML string: <div style=\"height: 30px; margin-left: -1.5em; margin-top: 2.5em; margin-bo...\n",
      "No content found in HTML string: <style type=\"text/css\">\n",
      "  ul {<br />\n",
      "    list-style-type: none;<br />\n",
      "    p...\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## NEXT NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: next notebook code\n",
    "# Go to 03_export.ipynb"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
